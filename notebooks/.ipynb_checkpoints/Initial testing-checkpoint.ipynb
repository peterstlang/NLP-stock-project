{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee2f1a5",
   "metadata": {},
   "source": [
    "# DEMO solution\n",
    "This notebook serves as first step in using NLP and machine learning to predict stock movements\n",
    "This first iteration will focus on scraping headlines using newsapi.org API and create a dataset for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ea253",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "Using newsapi.org to get headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbf8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting API-key\n",
    "\n",
    "import sys    \n",
    "\n",
    "sys.path.append(\"C:/Users/peter/Documents/NLP-stock-project\")\n",
    "\n",
    "from keys import APIkey\n",
    "\n",
    "# some more imports\n",
    "\n",
    "import pprint\n",
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49682d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define endpoint\n",
    "url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# parameters\n",
    "parameters = {\n",
    "    'q': 'microsoft', # query phrase\n",
    "    'from': '2023-10-08',  # Start date (YYYY-MM-DD) - specify your desired date here\n",
    "    'to': '2023-10-10',    # End date (YYYY-MM-DD) - specify your desired end date\n",
    "    'sortBy': 'publishedAt', # sort by publishingdate # number of articles (100 max)\n",
    "    'apiKey': APIkey # your own API key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a23b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     response = requests.get(url, params=parameters)\n",
    "#     response.raise_for_status()  # Raise HTTPError for bad requests\n",
    "#     response_json = response.json()\n",
    "#     print(response_json)\n",
    "# except requests.exceptions.HTTPError as errh:\n",
    "#     print(\"HTTP Error:\", errh)\n",
    "# except requests.exceptions.ConnectionError as errc:\n",
    "#     print(\"Error Connecting:\", errc)\n",
    "# except requests.exceptions.Timeout as errt:\n",
    "#     print(\"Timeout Error:\", errt)\n",
    "# except requests.exceptions.RequestException as err:\n",
    "#     print(\"OOps: Something Else\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f190ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the request\n",
    "response = requests.get(url, params=parameters)\n",
    "\n",
    "# Convert the response to JSON format\n",
    "response_json = response.json()\n",
    "\n",
    "# Check out the dictionaries keys\n",
    "#print(response_json.keys())\n",
    "\n",
    "# get articles\n",
    "articles = response_json[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4d3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4e7a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of results: 2038\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of results\n",
    "total_results = response_json[\"totalResults\"]\n",
    "\n",
    "# Print the total number of results\n",
    "print(f\"Total number of results: {total_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb7bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = list(map(lambda x: x[\"title\"], articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d4c435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pprint.pprint(response_json['articles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f05654",
   "metadata": {},
   "source": [
    "#### pagination loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32a0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing datetime\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "\n",
    "# bool\n",
    "flag = True\n",
    "\n",
    "# api endpoint\n",
    "url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# query params\n",
    "query = 'microsoft'\n",
    "start_date = date.today() -timedelta(days=14)\n",
    "end_date = date.today()\n",
    "sortBy = 'publishedAt'\n",
    "api = APIkey\n",
    "pagesize = 100\n",
    "\n",
    "# empty list for titles\n",
    "ms_titles = []\n",
    "\n",
    "# creating date var for loop\n",
    "current_date = start_date\n",
    "\n",
    "while current_date < end_date:\n",
    "    # to not crash API\n",
    "    time.sleep(1)\n",
    "    # api params\n",
    "    parameters = {\n",
    "        'q': query, # query phrase\n",
    "        'from': current_date,  # Start date (YYYY-MM-DD) - specify your desired date here\n",
    "        'to': current_date,    # End date (YYYY-MM-DD) - specify your desired end date\n",
    "        'sortBy': sortBy, # sort by publishingdate # number of articles (100 max)\n",
    "        'pageSize': pagesize,\n",
    "        'apiKey': api, # your own API key\n",
    "    }\n",
    "    \n",
    "    # calling the api and getting json\n",
    "    response = requests.get(url, params=parameters)\n",
    "    response_json = response.json()\n",
    "    # get articles\n",
    "    articles = response_json[\"articles\"]\n",
    "    \n",
    "    # getting titles\n",
    "    titles_per_page = map(lambda x: x[\"title\"], articles)\n",
    "    # extending list\n",
    "    ms_titles.extend(titles_per_page)\n",
    "    \n",
    "    # update while variable\n",
    "    current_date = current_date + timedelta(days=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f41158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702502c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Microsoft's quarterly results and guidance showcase its AI prowess\", 'Marketmind: A welcome bounce, but mixed big tech signals', 'Microsoft posts strong results on growing demand for AI services', \"Microsoft's AI bets boost cloud business, Alphabet yet to find silver lining\", 'Alphabet Revenues Up 11 Percent in Q3 on Stronger Ad Sales', 'Policía internacional detiene a grupo criminal que hackeó a Capcom', 'Cloud revenue miss drags on Alphabet’s stock, despite strong results overall', 'Galaxy Book 4 Ultra and Snapdragon X Elite can be a match made in heaven', 'デル、新開発のAzureハイブリッドクラウド基盤を発表', 'Microsoft รายงานผลประกอบการ เติบโตสูงโดยเฉพาะธุรกิจคลาวด์']\n"
     ]
    }
   ],
   "source": [
    "print(ms_titles[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5229e63",
   "metadata": {},
   "source": [
    "## Trying unsupervised learning models on my small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51123817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'original dataset: 1400 headlines, cleaned dataset: 809 headlines, 591 headlines removed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start by filtering out non-english headlines\n",
    "\n",
    "#import\n",
    "from langdetect import detect\n",
    "    \n",
    "headlines = ms_titles\n",
    "cleaned_headlines = [title for title in headlines if detect(title) == \"en\"]\n",
    "\n",
    "\"\"\"original dataset: {0} headlines, \n",
    "cleaned dataset: {1} headlines, {2} \n",
    "headlines removed\"\"\".format(len(headlines),\n",
    "                               len(cleaned_headlines),\n",
    "                               len(headlines) - len(cleaned_headlines)\n",
    "                               ).replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c32dc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pos': 250, 'neg': 135, 'neu': 424}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk.sentiments.vader.SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "score_dict = {\"pos\": 0, \"neg\": 0, \"neu\": 0}\n",
    "\n",
    "for headline in cleaned_headlines:\n",
    "    sentiment_scores = sid.polarity_scores(headline)\n",
    "    \n",
    "    #max_sentiment = max(sentiment_scores, key=lambda k: sentiment_scores[k])\n",
    "    \n",
    "    if sentiment_scores[\"compound\"] >= 0.1:\n",
    "        score_dict[\"pos\"] += 1\n",
    "    elif sentiment_scores[\"compound\"] <= -0.1:\n",
    "        score_dict[\"neg\"] += 1\n",
    "    else:\n",
    "        score_dict[\"neu\"] += 1\n",
    "        \n",
    "score_dict\n",
    "    \n",
    "    #for k in sorted(sentiment_scores):\n",
    "    #    print('{0}: {1}, '.format(k, sentiment_scores[k]), end='')\n",
    "    #print()\n",
    "#sentiment_scores[\"neu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e39a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"..\\data\\news_headlines.csv\" created successfully with header: ['Title', 'Description', 'Source', 'Published At']\n"
     ]
    }
   ],
   "source": [
    "#import csv\n",
    "#import os\n",
    "\n",
    "# Define the header for your CSV file\n",
    "#header = ['Title', 'Description', 'Source', 'Published At']\n",
    "\n",
    "# Specify the file path for your CSV file in the data folder\n",
    "#csv_file_path = os.path.join('..', 'data', 'news_headlines.csv')\n",
    "\n",
    "# Open the CSV file in write mode and write the header\n",
    "#with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#    csv_writer = csv.writer(csvfile)\n",
    "#    csv_writer.writerow(header)\n",
    "\n",
    "#print(f'CSV file \"{csv_file_path}\" created successfully with header: {header}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba62f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Documents\\NLP-stock-project\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
